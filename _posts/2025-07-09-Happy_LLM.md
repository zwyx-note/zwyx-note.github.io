---
title: Happy LLM - Datawhale
date: 2025-07-09 10:00:00 +0800
categories: [LLM/MLLM, Datawhale]
tags: [llm/mllm]     # TAG names should always be lowercase
description: Happy LLM - Datawhale
math: true
---

- [课程链接](https://datawhalechina.github.io/happy-llm/#/./README)

## 第一章：NLP 基础概念

- NLP 核⼼任务是通过计算机程序来模拟⼈类对语⾔的认知和使⽤过程

- 从符号主义与统计方法 到 机器学习与深度学习

- 任务包括但不限于中文分词、子词切分、词性标注、文本分类、实体识别、关系抽取、文本摘要、机器翻译以及自动问答系统的开发。

## 第二章：Transformer 架构

### 2.1 注意力机制

#### 2.1.1 什么是注意力机制

- 从 CV 为起源发展起来的神经网络，其核心架构有三种：

    - 前馈神经网络 Feedforward Neural Network, FNN；每层神经元之间完全链接；

    - 卷积神经网络 Convolutional Neural Network, CNN：用训练参数量远小于 FNN 的卷积层来进行特征提取和学习；

    - 循环神经网络 Recurrent Neural Network, RNN：能够用历史信息作为输入，包含环和自重复的网络；

- 过去常用 RNN，但 RNN 限制了计算机并行计算的能力，难以捕捉长序列的相关关系；

#### 2.1.2 深入理解注意力机制

- 于是借鉴 CV 的注意力机制，有了 Transformer：

    - 将重点注意力集中在一个或几个 token；

    - 注意力机制有三个核心变量：Query（查询值）、Key（键值）和 Value（真值）。

    - 注意力机制的特点是通过计算 Query 与 Key 的相关性为真值加权求和（注意力分数），从而拟合序列中每个词同其他词的相关关系。

#### 2.1.3 注意力机制的实现

$$
\begin{equation}
  attention(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
  \label{eq:attention}
\end{equation}
$$

- 其中，$ Q $ 与 $ K $ 相乘的结果反映了 $ query $ 与每一个 $ key $ 的相似程度，经 $ softmax $ 归一化，得到注意力分数。将其乘上 $ V $，得到最终值。 

- 除以 $ \sqrt{d_k} $ 是为了避免 $ Q $ 与 $ K $ 对应的维度 $ d_k $ 较大，使得 $ softmax $ 放缩时受过多影响，使不同值之间的差异较大，影响梯度的稳定性

```python
import torch
import math

def attention(query, key, value, dropout = None):

    # query  [batch, seq_len_q, dim]
    # key    [batch, seq_len_k, dim]
    d_k = query.size(-1) # 获取特征 dim

    # matmul 矩阵计算
    # transpose 使得 q 与 k 能相乘
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)

    # scores [batch, seq_len_q, seq_len_k]
    # softmax 使得 seq_len_k 的值归一化
    p_attn = scores.softmax(dim = -1)
    
    if dropout is not None:
        p_attn = dropout(p_attn)

    return torch.matmul(p_attn, value), p_attn
```

#### 2.1.4 自注意力机制 Self-attention

- 在 Transformer 的 Encoder 的，我们希望获取每一个 token 对其它 token 的注意力分布，就有了自注意力机制（Self-attention）

```python
attention(x, x, x)
```

#### 2.1.5 掩码自注意力机制 Mask Self-attention

- 让模型只能看到历史信息而看不到未来信息，需要用 \[mask\] 来遮蔽未来的信息

```bash
<BOS> [MASK] [MASK] [MASK] [MASK],
<BOS>   I     [MASK] [MASK] [MASK],
<BOS>  I      like  [MASK] [MASK],
<BOS>  I      like   you   [MASK],
<BOS>  I      like   you   </EOS>
```

- **掩码矩阵就是一个跟文本序列等长的上三角矩阵**，当输入维度为 （batch_size, seq_len, hidden_size）时，我们的 Mask 矩阵维度一般为 (1, seq_len, seq_len)（通过广播实现同一个 batch 中不同样本的计算）。

```python
# 掩码矩阵
mask = torch.full((1, args.max_seq_len, args.max_seq_len), float("-inf"))

# diagonal = 1 使得只保留对角线上的元素，若 = 0 则保留对角线元素；除上三角地区为 -inf 外，其它元素均为 0
mask = torch.triu(mask, diagonal = 1)

# 把负无穷掩码加上 softmax 对极大极小值敏感，-inf 概率会趋近于 0，实现屏蔽未来位置的注意力
scores = scores + mask[:, :seqlen, :seqlen]

# 最后一维归一化 使得类型同 xq
scores = F.softmax(scores.float(), dim = -1).type_as(xq)
```

#### 2.1.6 多头注意力机制 Multi-Head attention

- 一次注意力计算只能拟合一种相关关系，单一的注意力机制很难全面拟合语句序列里的相关关系。因此 Transformer 使用了多头注意力机制，同时对一个语料进行多次注意力计算，每次注意力计算都能拟合不同的关系，将最后的多次结果拼接起来作为最后的输出

$$
\begin{equation}
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
  \label{eq:multi_head}
\end{equation}
$$
其中，$ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $


- 我们可以通过矩阵运算巧妙地实现并行的多头计算，其核心逻辑在于使用三个组合矩阵来代替了 n 个参数矩阵的组合，也就是矩阵内积再拼接其实等同于拼接矩阵再内积。

```python
import torch.nn as nn
import torch
import torch.nn.functional as F
import math

class ModelArgs:
    def __init__(self, 
                 n_embd=768, 
                 n_heads=12, 
                 dim=768, 
                 dropout=0.1, 
                 max_seq_len=2048,
                 model_parallel_size=1):
        self.n_embd = n_embd
        self.n_heads = n_heads
        self.dim = dim
        self.dropout = dropout
        self.max_seq_len = max_seq_len
        self.model_parallel_size = model_parallel_size

class MultiHeadAttention(nn.Module):

    def __init__(self, args: ModelArgs, is_causal = False):
        super().__init__()

        # 隐藏层维度必须是头数的整数倍 后面我们会将输入拆成头数个矩阵
        assert args.n_embd % args.n_heads == 0, "隐藏层维度必须是头数的整数倍"
        # 模型并行处理大小
        model_parallel_size = 1
        # 本地计算头数
        self.n_local_heads = args.n_heads // model_parallel_size
        # 每个头的维度
        self.head_dim = args.dim // args.n_heads

        # Wq Wk Wv 三个参数矩阵 [n_embd * n_embd]
        # 三个组合矩阵来代替了 n 个参数矩阵的组合，矩阵内积再拼接相当于拼接矩阵再内积
        # 将输入维度 args.dim 投影到多头维度 args.n_heads * self.head_dim
        self.wq = nn.Linear(args.dim, args.n_local_heads * self.head_dim, bias = False)
        self.wk = nn.Linear(args.dim, args.n_local_heads * self.head_dim, bias = False)
        self.wv = nn.Linear(args.dim, args.n_local_heads * self.head_dim, bias = False)
        # 输出权重矩阵 [n_embd * n_embd] head_dim = n_embds / n_heads
        self.wo = nn.Linear(args.n_local_heads * self.head_dim, args.dim, bias = False)
        # 注意力与残差连接的 dropout
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)

        self.is_causal = is_causal

        # 一个上三角矩阵 mask 掩码
        # 在多头注意力下 mask 矩阵要比之前我们定义的多一个维度
        if self.is_causal:
            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float("-inf"))
            mask = torch.triu(mask, diagonal = 1)
            # 注册为模型的缓冲区
            # 缓冲区是一种特殊的张量，缓冲区不会被视为需要优化的参数，不会在反向传播中更新
            # 但仍随模型保存 通常用于固定数据
            self.register_buffer("mask", mask)

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):

        # 获取批次大小和序列长度 [batch_size, seq_len, dim]
        batch_size, seqlen, _ = q.shape

        # 计算查询（Q）、键（K）、值（V）,输⼊通过参数矩阵层，维度为 (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)
        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)

        #    [batch_size, seq_len, hidden_dim] 
        # -> [batch_size, seq_len, n_local_heads, head_dim] 
        # -> [batch_size, n_local_heads, seq_len, head_dim]
        xq = xq.view(batch_size, seqlen, self.n_local_heads, self.head_dim).transpose(1, 2)
        xk = xk.view(batch_size, seqlen, self.n_local_heads, self.head_dim).transpose(1, 2)
        xv = xv.view(batch_size, seqlen, self.n_local_heads, self.head_dim).transpose(1, 2)

        # 注意力计算
        # [B, nh, T, hs] * [B, nh, hs, T] -> [B, nh, T, T]
        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.n_local_heads)

        # 若为掩码 有
        if self.is_causal:
            assert hasattr(self, 'mask'), "不存在掩码"
            scores = scores + self.mask[:, :, :seqlen, :seqlen]

        # softmax [B, nh, T, T] 后 dropout
        scores = F.softmax(scores.float(), dim = -1).type_as(xq)
        scores = self.attn_dropout(scores)

        # scores * V
        # [B, nh, T, T] * [B, nh, T, hs] -> [B, nh, T, hs]
        output = torch.matmul(scores, xv)

        # 恢复时间维度并合并头
        
        # 将多头的结果拼接起来 C = n_local_heads × head_dim, C 即隐藏层维度 hidden_dim

        # [B, T, n_local_heads, C // n_local_heads] -> [B, T, n_local_heads * C // n_local_heads]

        # contiguous 函数用于重新开辟一块新内存存储,因为 Pytorch 设置先 transpose 再 view 会报错,

        # 因为 view 直接基于底层存储得到,然而 transpose 并不会改变底层存储,因此需要额外存储
        
        # view(bsz, seqlen, -1) 会保持张量元素总数不变，将张量重新组织成 [bsz, seqlen, ...] 的形状：
        output = output.transpose(1, 2).contiguous().view(batch_size, seqlen, -1)

        # 投影回残差流
        output = self.wo(output)
        output = self.resid_dropout(output)
        return output
```

### 2.2 Encoder - Decoder

#### 2.2.1 Seq2Seq 模型

- 在 Transformer 中，使用注意力机制的是其两个核心组件 —— Encoder（编码器）和 Decoder（解码器）

- 所谓编码，就是将输入的自然语言序列通过隐藏层编码成能够表征语义的向量（或矩阵），可以简单理解为更复杂的词向量表示。而解码，就是对输入的自然语言序列编码得到的向量或矩阵通过隐藏层输出，再解码成对应的自然语言目标序列。

- Seq2Seq 是 NLP 最经典的任务，几乎所有的 NLP 任务都可以视为 Seq2Seq 任务。

- Transformer 由 Encoder 和 Decoder 组成，每一个 Encoder（Decorder）又由 6 个 Encoder（Decoder）Layer 组成。输入源序列会进入 Encoder 进行编码，到 Encoder Layer 的最顶层再将编码结果输出给 Decoder Layer 的每一层，通过 Decoder 解码后就可以得到输出目标序列了 

#### 2.2.2 前馈神经网络 Feed Forward Neural Network

- Transformer 中的 FFN 步骤：$ w1 -> relu -> w2 -> dropout $

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    
    def __init__(self, dim : int, hidden_dim : int, dropout : float):
        super.__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias = False)
        self.w2 = nn.Linear(hidden_dim, dim, bias = False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # w1 -> relu -> w2 -> dropout
        return self.dropout(self.w2(F.relu(self.w1(x))))
```

#### 2.2.3 层归一化 Layer Norm

- 假设输入是一个批次的图像 $ [batchsize, channels, height, width] $：

    - BN：对每个通道（如 RGB 通道），计算所有样本在该通道上的均值和方差，然后归一化。同一特征列的不同样本被归一化（跨样本），不同特征列独立计算。
    
    - LN：对每个图像单独计算所有通道的均值和方差，然后归一化。同一样本内的所有特征被归一化（样本内），不同样本独立计算。

    - 假设一个批次 $ [batchsize=2, features=3] $，

    $$
    x = 
    \begin{bmatrix} 
    1.0 & 2.0 & 3.0 \\ 
    4.0 & 5.0 & 6.0 
    \end{bmatrix}
    $$

    - BN 结果为（特征列被归一化）

    $$
    \widetilde{x}_{\text{BN}} = 
    \begin{bmatrix} 
    -1.0 & -1.0 & -1.0 \\ 
    1.0 & 1.0 & 1.0 
    \end{bmatrix}
    $$

    - LN 结果为（样本行被归一化）

    $$
    \widetilde{x}_{\text{LN}} = 
    \begin{bmatrix} 
    -1.2247 & 0.0 & 1.2247 \\ 
    -1.2247 & 0.0 & 1.2247 
    \end{bmatrix}
    $$

-  归一化公式：

$$
\begin{equation}
    \widetilde{Z}_j = \frac{Z_j - \mu_j}{\sqrt{\sigma^2 + \epsilon}}
    \label{eq:batch_norm_layer_norm_formula}
\end{equation}
$$

```python
class LayerNorm(nn.Module):

    def __init__(self, features, eps = 1e-6):
        super(LayerNorm, self).__init__()

        # nn.Parameter 会将张量注册为模块的可学习参数，反向传播时会自动更新
        # 初始值为形状 [features] 的全 1 张量，归一化后缩放分布
        self.a_2 = nn.Parameter(torch.ones(features))
        # 初始值为形状 [features] 的全 0 张量，归一化后偏移分布
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):

        # mean std 形状为 [bsz, max_len, 1]
        # keepdim 会保留指定的维度，而不是丢弃，若丢弃则变为 [bsz, max_len]
        mean = x.mean(-1, keepdim = True)
        std = x.std(-1, keepdim = True)

        # 最后一维为 1，进行了广播
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

#### 2.2.4 残差连接

- 由于 Transformer 模型结构较复杂、层数较深，​为了避免模型退化，Transformer 采用了残差连接的思想来连接每一个子层。

- 残差连接，即下一层的输入不仅是上一层的输出，还包括上一层的输入。残差连接允许最底层信息直接传到最高层，让高层专注于残差的学习。

$$
\begin{equation}
    x = x + \text{MultiHeadSelfAttention}(\text{LayerNorm}(x))
\end{equation}
$$

$$
\begin{equation}
    \text{output} = x + \text{FNN}(\text{LayerNorm}(x))
\end{equation}
$$

```python
h = x + self.attention.forward(self.attention_norm(x))
output = h + self.feed_forward.forward(self.fnn_norm(h))
```

#### 2.2.5 Encoder 编码器

- Encoder 由 N 个 Encoder Layer 组成，每一个 Encoder Layer 包括一个多头自注意力层和一个前馈神经网络。

- 一个 Encoder Layer：

```python
class EncoderLayer(nn.Module):

    def __init__(self, args):
        super().__init__()

        # 一个 Layer 中有两个 norm，分别在 attention 与 MLP 之前
        self.attention_norm = LayerNorm(features = args.n_embd)
        # Encoder 不需要掩码
        self.attention = MultiHeadAttention(args, is_causal = False)
        self.fnn_norm = LayerNorm(features = args.n_embd)
        self.feed_forward = MLP(args)

    def forward(self, x):
        # Layer Norm
        norm_x = self.attention_norm(x)
        # 自注意力层
        h = x + self.attention.forward(norm_x, norm_x, norm_x)
        # 经过前馈
        output = h + self.feed_forward.forward(self.fnn_norm(h))

        return output
```

- 若干 Encoder Layer 组成一个 Encoder：

```python
class Encoder(nn.Module):

    def __init__(self, args):
        super(Encoder, self).__init__()

        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.n_layer)])
        self.norm = LayerNorm(args.n_embd)

    def forward(self, x):
        # 分别通过 N 层 Encoder Layer
        for layer in self.layers:
            x = layer(x)
        
        return self.norm(x)
```

#### 2.2.6 Decoder 解码器

- N 个 Decoder Layer 组装为 Decoder，Decoder 由两个注意力层和一个前馈神经网络组成。

    - 第一个注意力层是一个掩码自注意力层；
    
    - 第二个注意力层是一个多头注意力层，该层将使用第一个注意力层的输出作为 query，使用 Encoder 的输出作为 key 和 value，来计算注意力分数。
    
    - 最后，再经过前馈神经网络：

- 一个 Decoder Layer：

```python
class DecoderLayer(nn.Module):

    def __init__(self, args):
        super().__init__()

        self.attention_norm1 = LayerNorm(args.n_embd)
        self.mask_attetion = MultiHeadAttention(args, is_causal=True)

        self.attention_norm2 = LayerNorm(args.n_embd)
        self.attention = MultiHeadAttention(args, is_causal=False)

        self.ffn_norm = LayerNorm(args.n_embd)
        self.feed_forward = MLP(args)

    def forward(self, x, encoder_output):

        # 掩码自注意力
        norm_x = self.attention_norm1(x)
        x = x + self.mask_attetion.forward(norm_x, norm_x, norm_x)

        # 多头注意力
        norm_x = self.attention_norm2(x)
        h = x + self.attention.forward(norm_x, encoder_output, encoder_output)

        # 前馈
        output = h + self.feed_forward.forward(self.ffn_norm(h))

        return output
```

- 若干个 Decoder Layer 组成一个 Decoder：

```python
class Decoder(nn.Module):

    def __init__(self, args):
        super(Decoder, self).__init__()

        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.n_layer)])
        self.norm = LayerNorm(args.n_embd)

    def forward(self, x, encoder_output):

        for layer in self.layers:
            x = layer(x, encoder_output)

        return self.norm(x)
```

> Encoder Layer 为 多头自注意力 + 前馈；Decoder Layer 为 掩码多头自注意力 + 编码解码交叉多头注意力 + 前馈。
{: .prompt-info }

### 2.3 搭建一个 Transformer

#### 2.3.1 Embedding 层

- 自然语言通过分词器 tokenizer，切分成 token 并转化为一个固定的 index。

- Embedding 层负责将自然语言的输入转化为机器可以处理的向量。Embedding 层其实是一个存储固定大小的词典的嵌入向量查找表。Embedding 层的输入往往是一个形状为 $ [batchsize, seqlen, 1] $ 的矩阵。其中最后一层的维度为 1，代表已经转换好的 index。

```python
self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)

# num_embeddings：词汇表大小（即词表中唯一词的数量）
# embedding_dim：每个词向量的维度

# 创建一个包含1000个词、每个词用50维向量表示的嵌入层
embedding = nn.Embedding(num_embeddings=1000, embedding_dim=50)
```

#### 2.3.2 位置编码

- 注意力机制可以实现良好的并行计算，但也导致序列中相对位置的丢失。对于序列中的每一个 token，其他各个位置对其来说都是平等的。故引入位置编码来进行编码，再将位置编码加入原先的词向量编码中。

$$
x = x + x_{PE}
$$

- Transformer 使用了正余弦函数来进行位置编码（绝对位置编码 Sinusoidal），其编码方式为：

$$
\begin{equation}
    PE(\text{pos}, 2i) = \sin\left( \frac{\text{pos}}{10000^{2i/d_{\text{model}}}} \right)
    \label{eq:PE_Sinusoidal_sin}
\end{equation}
$$

$$
\begin{equation}
    PE(\text{pos}, 2i + 1) = \cos\left( \frac{\text{pos}}{10000^{2i/d_{\text{model}}}} \right)
    \label{eq:PE_Sinusoidal_cos}
\end{equation}
$$

- numpy 即可快速实现：

```python
import numpy as np

def PositionEncoding(seq_len, d_model, n = 10000):
    P = np.zeros((seq_len, d_model))

    for k in range(seq_len):
        for i in np.arange(int(d_model / 2)):
            denominator = np.power(n, 2 * i / d_model)
            P[k, 2 * i] = np.sin(k / denominator)
            P[k, 2 * i + 1] = np.cos(k / denominator)

    return P

P = PositionEncoding(seq_len = 4, d_model = 4, n = 100)
print(P)

'''
[[ 0.          1.          0.          1.        ]
 [ 0.84147098  0.54030231  0.09983342  0.99500417]
 [ 0.90929743 -0.41614684  0.19866933  0.98006658]
 [ 0.14112001 -0.9899925   0.29552021  0.95533649]]
'''
```

- 绝对位置编码 Sinusoidal 的好处：

    - 使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。

    - 容易地计算出相对位置，对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到，三角函数易得。

- 基于以上原理，实现一个位置编码层：

```python
class PositionalEncoding(nn.Module):

    def __init__(self, args):
        super().__init__()

        self.dropout = nn.Dropout(p = args.dropout)

        # block_size 是序列的最大长度
        pe = torch.zeros(args.block_size, args.n_embd)

        # unsqueeze(1) 在维度 1 上增加一个维度，使其形状变为 [args.block_size, 1]
        position = torch.arange(0, args.block_size).unsqueeze(1)

        div_term = torch.exp(
            torch.arange(0, args.n_embd, 2) * -(math.log(10000.0) / args.n_embd)
        )

        # 分别计算 sin cos 结果
        pe[:, 0::2] = torch.sin(position / div_term)
        pe[:, 1::2] = torch.cos(position / div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        
        # [batch_size, seq_len, ...]
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)

        return self.dropout(x)
```

- 注：其中存在数学变化：

$$
\frac{1}{10000^{\frac{2i}{d_{\text{model}}}}} = 10000^{-\frac{2i}{d_{\text{model}}}}
$$

$$
= \exp\left( \ln\left(10000^{-\frac{2i}{d_{\text{model}}}}\right) \right)
$$

$$
= \exp\left( -\frac{2i}{d_{\text{model}}} \cdot \ln(10000) \right)
$$

$$
= \exp\left( 2i \cdot \left( -\frac{\ln(10000)}{d_{\text{model}}} \right) \right)
$$

#### 2.3.3 一个完整的 Transformer

- ![结构图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_01.png)

- 上图是原论文 [Attention is all you need](https://arxiv.org/abs/1706.03762) 配图，LayerNorm 层放在了 Attention 层后面，也就是“Post-Norm”结构。

    - 但在其发布的源代码中，LayerNorm 层是放在 Attention 层前面的，也就是“Pre Norm”结构。
    
    - 考虑到目前 LLM 一般采用“Pre-Norm”结构（可以使 loss 更稳定），本文在实现时采用“Pre-Norm”结构。

```python
class Transformer(nn.Module):
    '''整体模型'''
    def __init__(self, args):
        super().__init__()

        # 必须输入词表大小和 block size
        assert args.vocab_size is not None, "vocab_size should not be None!"
        assert args.block_size is not None, "block_size should not be None!"
        self.args = args
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(args.vocab_size, args.n_embd),
            wpe = PositionalEncoding(args),
            drop = nn.Dropout(args.dropout),
            encoder = Encoder(args),
            decoder = Decoder(args),
        ))

        # 最后的线性层 n_embd -> vocab_size
        self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias = False)

        # 初始化所有权重
        # self.apply(fn)是nn.Module提供的一个重要方法。这个方法的作用是递归地将函数fn应用到当前模块及其所有子模块上 
        self.apply(self._init_weights)

        # 查看所有参数的数量
        print("number of parameters : %.2fM" % (self.get_num_params() / 1e6))

    '''统计所有参数的数量'''
    def get_num_params(self, non_embedding = False):

        # non_embedding: 是否统计 embedding 的参数
        # numel() 是张量 Tensor 的一个方法，用于返回张量中元素的总数，每个参数（如权重矩阵、偏置向量）都是一个张量
        n_params = sum(p.numel() for p in self.parameters())

        # 若不统计 embedding 就减去
        if non_embedding:
            n_params -= self.transformer.wpe.weight.numel()
            
        return n_params
    
    '''初始化权重'''
    def _init_weights(self, module):

        # 线性层和 Embedding 层初始化为正则分布
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean = 0.0, std = 0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean = 0.0, std = 0.02)

        # isinstance() 是一个内置函数，isinstance(object, classinfo)，
        # 如果 object 是 classinfo 的实例或子类实例，返回 True，否则返回 False
        # self.parameters() 返回一个生成器（Generator），遍历模型中所有可训练参数（即需要计算梯度的 nn.Parameter 对象）。

    '''前向计算函数'''
    def forward(self, idx, targets = None):

        # 输入为 idx，维度为 [batch_size, seq_len, 1]；
        # targets 为目标序列，用于计算 loss
        device = idx.device
        b, t = idx.size()
        assert t <= self.args.block_size, f"不能计算该序列，该序列长度为 {t}, 最大序列长度只有 {self.args.block_size}"

        # 通过 self.transformer
        print("idx", idx.size())
        # idx -> Embedding 层，[batch_size, seq_len, n_embd]
        tok_emb = self.transformer.wte(idx)
        print("tok_emb", tok_emb.size())
        # -> 位置编码层
        pos_emb = self.transformer.wpe(tok_emb)
        # -> Dropout
        x = self.transformer.drop(pos_emb)
        # -> Encoder
        print("x after wpe:", x.size())
        encoder_out = self.transformer.encoder(x)
        print("encoder_out:", encoder_out.size())
        # -> Decoder
        x = self.transformer.decoder(x, encoder_out)
        print("x after decoder:", x.size())

        if targets is not None:
            # 训练阶段，如果我们给了 targets，就计算 loss
            # 先通过最后的 Linear 层，得到维度为 [batch_size, seq_len, vocab_size]
            logits = self.lm_head(x)
            # 与 targets 计算交叉熵
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1)
        else:
            # 推理阶段，我们只需要 logits，loss 为 None
            # 取 -1 是只取序列中的最后一个作为输出
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            loss = None

        return logits, loss
```

> 如果在继承了 `nn.Module` 的类中定义了 `forward()`，可以通过 `model(x)` 来直接调用 `forward()`，而无需显式调用 `forward()`
{: .prompt-tip }

> `self.apply(fn)` 是 `nn.Module` 提供的一个重要方法。这个方法的作用是递归地将函数 `fn` 应用到当前模块及其所有子模块上 
{: .prompt-tip }

> `self.parameters()` 返回 `Generator`，遍历模型中所有可训练参数（即需要计算梯度的 `nn.Parameter` 对象）。
{: .prompt-tip }

## 第三章：预训练语言模型

### 3.1 Encoder-only PLM（Pretrained LM）

- 针对 Encoder、Decoder 的特点，引入 ELMo 的预训练思路，开始出现不同的、对 Transformer 进行优化的思路。

#### 3.1.1 BERT

- BERT，全名为 Bidirectional Encoder Representations from Transformers，是由 Google 团队在 2018年发布的预训练语言模型。该模型发布于论文 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/)

- BERT 沿袭的核心思想包括：Transformer 与 预训练 + 微调 范式（2018年，ELMo 的诞生标志着预训练+微调范式的诞生）

- ![BERT 模型架构](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_02.png)

- BERT 是针对于 NLU Natural Language Understanding 任务打造的预训练模型，输入为文本序列，输出是 label。

- BERT = Embedding + Encoder + prediction_heads
    
    - 为了让 Transformer 这个 Seq2seq 适配 NLU，故引入一个分类头 prediction_heads。

    - Prediction_heads = Linear + activation func + Linear

- Encoder Layer 与 Transformer 中的 Encoder 类似

    - Embedding + attention + 残差 + intermediate 层。Intermediate 层是 BERT 的特殊称呼，即 Linear + activation func

- BERT 所用的激活函数是 GELU 函数，即“高斯误差线性单元激活函数”，这也是自 BERT 才开始被普遍关注的激活函数。
    
    - GELU 的核心思路为将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元。

$$
\begin{equation}
    GELU(x) = 0.5x \left(1 + \text{tanh}\left( \sqrt{\frac{2}{\pi}} \left( x + 0.044715x^3 \right) \right) \right)
    \label{eq:GELU}
\end{equation}
$$

- 在注意力机制处，BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数。

    - 优点：可以拟合更丰富的相对位置信息

    - 缺点：增加不少模型参数，无法处理超训练长度的输入

- **BERT 提出两大创新预训练任务：MLM（Masked Language Model）与 NSP（Next Sentence Prediction）**

    - 一次大规模预训练后，通过小规模微调即可应用在下游任务中

    - 预训练数据的核心要求即是需要极大的数据规模，人工标记不显示，可通过遮蔽下文，使用上文预测下文即可。

    - 缺点：拟合从左到右的语义关系，忽略了双向的语义关系

- 提出 MLM，其核心思路为：随机遮蔽部分 token，将未被遮蔽的 token 输入，预测被遮蔽的 token，例如：

```md
输入：I <MASK> you because you are <MASK>
输出：<MASK> - love; <MASK> - wonderful
```

- 问题：但在下游任务微调和推理时，其实是不存在我们人工加入的 `<MASK>` 的，预训练与微调不一致会影响模型微调性能

    - 解决办法：随机选择 15% 的 token 用于遮蔽；这 15% 的 token 中，有 80% 的概率被遮蔽，10% 的概率被替换为任意一个 token，还有 10% 的概率保持不变

    - 原理：10% 保持不变是为了消除预训练和微调的不一致，而 10% 的随机替换核心意义在于迫使模型保持对上下文信息的学习。

    - 保留 10% 的原 token，模型看到一个 token “苹果”，它既可能是原句中的正确 token，也可能是被 mask 后未替换的 token。

    - 引入 10% 的随机替换后， “猫喜欢吃鱼” 中，“鱼” 被随机替换为 “狗”，模型看到的是 “猫喜欢吃狗”，此时它需要判断 “狗” 是否合理。

- 另一个预训练任务 NSP

    - 针对句级的 NLU 任务，如问答匹配、自然语言推理等；用来训练模型在句级的语义关系拟合。

    - 只需要从无监督语料中随机抽取连续句子或打乱，即可做到。

```md
输入：
    Sentence A：I love you.
    Sentence B: Because you are wonderful.
输出：
    1（是连续上下文）

输入：
    Sentence A：I love you.
    Sentence B: Because today's dinner is so nice.
输出：
    0（不是连续上下文）
```

- 为了能够快速迁移到下游任务，BERT 设计了更通用的输入和输出层来适配多任务下的迁移学习。

    - 对每一个输入的文本序列，BERT 会在其首部加入一个特殊 token `<CLS>`。在后续编码中，该 token 代表的即是整句的状态，也就是句级的语义表征。

    - `<CLS>`对应的输出向量会被模型 “训练” 成整个句子的全局语义表征，微调时只需拿 `<CLS>` 作为输入即可。

#### 3.1.2 RoBERTa

- 优化思想：大即好。

- 优化1：去掉 NSP 与训练任务，并将 Mask 操作放到训练阶段，实现每一个 Epoch 得到训练数据 Mask 的位置都不一致，即动态遮蔽策略；

- 优化2：更大规模的预训练数据与预训练步长，成为 LLM 诞生的基础之一；

- 优化3：更大的 bpe 词表，Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。

#### 3.1.3 ALBERT

- 优化思想：小而美。

- 优化1：将 Embedding 参数进行分解，在 Embedding 层的后面加入一个线性矩阵。Embedding 层的参数从 $ V * H $ 降低到了 $ V * E + E * H $。

- 优化2：跨层进行参数共享，让各个 Encoder 层共享模型参数，来减少模型的参数量。仅初始化一个 Encoder 层，实现进行 24 次计算，但只经过这一个 Encoder 层。

    - 不足：训练与推理速度相较 BERT 更慢，这也是 ALBERT 最终**没能取代 BERT 的一个重要原因**。

- 优化3：提出 SOP 预训练任务（Sentence-Order Prediction），即在 NSP 任务基础上打乱基础，学习其顺序关系。

```md
输入：
    Sentence A：I love you.
    Sentence B: Because you are wonderful.
输出：
    1（正样本）

输入：
    Sentence A：Because you are wonderful.
    Sentence B: I love you.
输出：
    0（负样本）
```

### 3.2 Encoder-Decoder PLM

#### 3.2.1 T5（Text-To-Text Transfer Transformer）

- T5 是由 Google 提出的一种 PLM，将所有 NLP 任务统一表示为文本到文本的转换问题。

- 模型架构：Encoder-Decoder

    - ![T5 模型架构](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_03.png)

    - Transformer 部分又分为 EncoderLayers 和 DecoderLayers 两部分，分别由一个个小的 Block 组成，每个 Block 包含多头注意力机制、前馈和 Norm 层，模型更加灵活。

    - T5 的 Self-Attention 机制与 BERT 的 Self-Attention 机制一样。

    - T5 的 LayerNorm 采用了 RMSNorm，通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。仅一个可调参数，适应度更好，比 Layer Norm \eqref{eq:batch_norm_layer_norm_formula} 更简单，同时有助于通过确保权重的规模不会变得过大或过小来稳定学习过程。

$$
\begin{equation}
    \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2 + \epsilon}} \cdot \gamma
    \label{eq:rms_norm}
\end{equation}
$$

- 其中：

    - $ x_i $ 是输入向量的第 $i$ 个元素

    - $ \gamma $ 是可学习的缩放参数

    - $ n $ 是输入向量的维度数量

    - $ \epsilon $ 是一个小常数，用于数值稳定性（以避免除以零的情况）

- 预训练任务：

    - T5 的预训练任务是 MLM，也成为 BERT-style 目标（遮蔽 15% 的 token......）

- **大一统思想**：

    - 设计理念是将所有不同类型的 NLP 任务转换为一个统一的格式：输入和输出都是纯文本。**其思想影响深远**。

    - 其预训练 + 微调的方式与 BERT、GPT 类似，但文本到文本的形式使得其适应性更强。

    - ![T5 大一统思想](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_04.png)

    - 对于不同的 NLP 任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。有利于模型在预训练阶段学习到不同任务之间的通用特征，也便于在微调阶段迅速适应具体任务。

### 3.3 Decoder-Only PLM

- Decoder-Only 就是目前大火的 LLM 的基础架构，目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）。

#### 3.3.1 GPT（Generative Pre-Training Language Model）

- GPT 是由 OpenAI 团队于 2018 年发布的预训练语言模型。

- BERT 是预训练语言模型时代的代表，而 GPT 首先明确提出了 预训练 - 微调 思想；

    - 通用预训练：在海量无监督语料上预训练，进而在每个特定任务上进行微调

- 模型架构：Decoder Only

    - ![GPT 模型架构图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_05.png)

    - GPT 使用的是 Sinusoidal 位置编码，即通过三角函数进行绝对位置编码，参考 \eqref{eq:PE_Sinusoidal_sin} 和 \eqref{eq:PE_Sinusoidal_cos}。

    - GPT 的 Decoder 层仅保留了一个掩码自注意力层，使用 Pre-Norm 归一化办法。

- 预训练任务：CLM（Casual Language Model）

    - 其思路为基于前 N 个 token 来预测下一个 token。

    - CLM 是更直接的预训练任务，其天生和人类书写自然语言文本的习惯相契合，与下游任务直接匹配。

```md
input: 今天天气
output: 今天天气很

input: 今天天气很
output：今天天气很好
```
- GPT 系列模型发展：

    - 核心思想：力大砖飞。

    - GPT-1 参数与 BERT 类似，但表现不如 BERT，这是 GPT 没有成为预训练语言模型时代的代表的原因。

    - GPT-2 以 zero-shot（零样本学习）为主要目标，同时从 Post-Norm 变为 Pre-Norm。

    - GPT-3 提出 few-shot（少样本学习），即在 prompt中增加 3~5个示例，来帮助模型理解，也被称为上下文学习（In-Context Learning），效果远好于 zero-shot，效率远高于传统的 预训练 - 微调 范式。

| 模型   | Decoder Layer | Hidden_size | 注意力头数 | 注意力维度 | 总参数量 | 预训练语料 |
| ------ | ------------- | ----------- | ---------- | ---------- | -------- | ---------- |
| GPT-1  | 12            | 3072        | 12         | 768        | 0.12B    | 5GB        |
| GPT-2  | 48            | 6400        | 25         | 1600       | 1.5B     | 40GB       |
| GPT-3  | 96            | 49152       | 96         | 12288      | 175B     | 570GB      |

#### 3.3.2 LLaMA

- LLaMA 模型是由 Meta（前 Facebook）开发的一系列大型预训练语言模型。

- 模型架构：Decoder Only

    - ![LLaMA 模型架构图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_06.png)

    - 经过多个 decoder block 的处理，hidden_states 会通过一个线性层进行最终的映射，这个线性层的输出维度与词表维度相同。这样，模型就可以根据 hidden_states 生成目标序列的概率分布，进而通过采样或贪婪解码等方法，生成最终的输出序列。

- LLaMA 模型的发展历程：

    - 开源，技术创新、多参数版本、大规模预训练和高效架构设计；

    - LLaMA-2 和 LLaMA- 3进一步通过引入分组查询注意力机制；

    - 训练数据大、词表大、支持长文本输入。

#### 3.3.3 GLM（Generative Language Model）

- 在整体模型结构上，GLM 和 GPT 大致类似，均是 Decoder-Only 的结构，仅有三点细微差异：

    - 使用 Post-Norm 而不是 Pre-Norm：Post Norm 在残差之后做归一化，对参数正则化的效果更强，模型的鲁棒性更好；Pre Norm 有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模型的梯度爆炸或者梯度消失。主流 LLM 都用 Pre-Norm。

    - 使用单个线性层实现最终 token 的预测，而不是使用 MLP；这样的结构更加简单也更加鲁棒，即减少了最终输出的参数量，将更大的参数量放在了模型本身；

    - 激活函数从 ReLU 换成了 GeLU，参考 \eqref{eq:GELU}。

- 预训练任务 - GLM（Generative Language Model）

    - GLM 是一种结合了自编码思想和自回归思想的预训练方法。

    - 自编码思想：MLM 的任务学习思路，在输入文本中随机删除连续的 tokens，要求模型学习被删除的 tokens；
    
    - 自回归思想：传统 CLM 任务学习思路，要求模型按顺序重建连续 tokens。

    - GLM 通过优化一个自回归空白填充任务来实现 MLM 与 CLM 思想的结合：与 MLM 遮蔽单个 token 不同，GLM 选择遮蔽一连串 token。

    - 既需要使用遮蔽部分的上下文预测遮蔽部分，在遮蔽部分内部又需要以 CLM 的方式完成被遮蔽的 tokens 的预测。

    - GLM 在预训练时代有优势，但在 LLM 时代，CLM 优势远超 MLM，从 ChatGLM2 开始，也回归了 CLM。

```md
输入：I <MASK> because you <MASK>
输出：<MASK> - love you; <MASK> - are a wonderful person
```

- GLM 家族发展

    - ChatGLM-6B：第一个中文开源 LLM；

    - ChatGML2-6B：上下文长度更长，更大预训练规模，回归 LLaMA 架构，引入 MQA（Multi-Query Attention）的注意力机制，回归 CLM 预训练任务；

    - ChatGLM3-6B：开始支持函数调用和代码解释器，可直接进行 Agent 开发；

    - ChatGLM4-9B：更大更好。

## 第四章：大语言模型

### 4.1 什么是 LLM

#### 4.1.1 LLM 的定义

- 参数数百亿（B billion 十亿），数 T token 语料，有**涌现能力**。

#### 4.1.2 LLM 的能力

- 涌现能力 Emergent Abilities

    - 在同样的模型架构与预训练任务下，大模型的某些能力远超小型模型，超过了随机水平，量变引起质变。

- 上下文学习 In-context Learning

    - 在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。

    - 微调对显存和训练样本仍有门槛要求，而 In-context Learning 可以大大节省。

    - 通过调整 Prompt 或提供 1-5 个自然语言示例，就可以令 GPT-4 超过传统 PLM 微调。

- 指令遵循 Instruction Following

    - IF 后的 LLM 可以理解并遵循未见指令执行任务，在同样使用指令形式化描述的未见过的任务上表现良好。泛化能力极强。

- 逐步推理 Step by Step Reasoning

    - LLM 通过思维链（Chain-of-Thought, CoT），揭示包含中间推理步骤的提示机制。

    - 这种能力可能是通过对代码的训练获得的。

#### 4.1.3 LLM 的特点

- 多语言支持

    - 预训练语料天然给 LLM 带来多语言能力。但需注意，这跟语料关系很大；如英文高质量语料占大部分，这使得 GPT-4 英文能力远超中文能力。

- 长文本处理

    - 在海量分布式训练集群上训练，LLM 在训练时就支持 4K 8K 32K 等上下文长度；

    - 可以使用旋转位置编码（Rotary Positional Encoding, RoPE），使得 LLM 具有长度外推能力，推理时能够处理显著长于训练长度的文本。

- 拓展多模态

    - 可以增加额外的参数；引入 Adapter 层和图像编码器，并针对性地在图文数据上进行有监督微调。

- 幻觉

    - 瞎编乱编；提示词工程和 RAG 只能减弱幻觉而无法彻底根除。

### 4.2 如何训练一个 LLM

- 训练 LLM 的三个阶段，如图所示：

- ![训练 LLM 的三个阶段](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_07.png)

#### 4.2.1 Pretrain

- 参数大、所需预训练语料多、所需算力资源极其庞大。

- 只能使用分布式训练框架：核心思路为 数据并行 + 模型并行；

- 数据并行：训练模型的尺寸可以被单个 GPU 内存容纳，但是 batch_size 放不进去；

- 下图展现了 模型、数据并行，让模型实例在不同 GPU 和不同批数据上运行。在数据并行的情况下，每张 GPU 上的模型参数是保持一致的，训练的总批次大小等于每张卡上的批次大小之和。

- ![数据并行与模型并行](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_08.png)

- 若模型参数量更大，单张 GPU 放不下，就把模型拆分到多个 GPU 上，每个 GPU 上存放不同的层或不同的部分。

- ![模型并行](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_09.png)

- 更高效的分布式方式，例如张量并行、3D 并行、ZeRO（Zero Redundancy Optimizer，零冗余优化器）等。

- 主流的分布式训练框架包括 Deepspeed、Megatron-LM、ColossalAI 等，其中，Deepspeed 使用面最广。

- Deepspeed 和核心策略是 ZeRO 和 CPU-offload。ZeRO 是一种显存优化的数据并行方案，其核心思想是优化数据并行时每张卡的显存占用。

    - 每张卡被占用的显存可分为“模型状态 Model States” + “剩余状态 Residual States”

    - 模型状态：包括模型参数、模型梯度和优化器 Adam 的状态参数。其中 Adam 占用存储空间最多；

    - 剩余状态：除模型状态外的。

- ZeRO 三种优化策略：

    - ZeRO-1：切 Adam 状态参数；

    - ZeRO-2：切 Adam 状态参数 + 模型梯度；

    - ZeRO-3：切 Adam 状态参数 + 模型梯度 + 模型参数；

    - 分片增加，训练中通信开销也增加；每张卡的 GPU 利用率 ZeRO-1 最高而 ZeRO-3 最低。

- 语料集：

    - 质量很重要，如何配比也很重要。

    - 文档准备 -> 语料过滤 -> 语料去重。

#### 4.2.2 SFT

- 不微调，LLM 就无法与其他下游任务或是用户指令适配；

- 故采取指令微调 -> 从指令中获得泛化的指令遵循能力。

    - 数据量数 B token 为好。

    - 配比很重要，可以关注 OpenAI 训练的 [InstructGPT](https://arxiv.org/abs/2203.02155)。

    - 可以通过 ChatGPT 或 GPT-4 来生成指令数据集，例如 [Alpaca](https://github.com/yizhongw/self-instruct/blob/main/human_eval/user_oriented_instructions.jsonl)

    - 为使模型学习到和预训练不同的范式，会设置特定格式，如下：

```md
### Instruction:\n{{content}}\n\n### Response:\n
```

- 指令微调本质上仍然是对模型进行 CLM 训练。

- 模型的多轮对话能力逐渐受到重视；

    - 模型是否支持多轮对话，与预训练没有关系，其能力完全来自于 SFT 阶段，必须构造多轮对话格式。

1. 直接将最后一次模型回复作为输出，前面所有历史对话作为输入，直接拟合最后一次回复：

```
input=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
output=[MASK][MASK][MASK][MASK][MASK]<completion_3>
```

2. 将 N 轮对话构造成 N 个样本：

```
input_1 = <prompt_1><completion_1>
output_1 = [MASK]<completion_1>

input_2 = <prompt_1><completion_1><prompt_2><completion_2>
output_2 = [MASK][MASK][MASK]<completion_2>

input_3=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
output_3=[MASK][MASK][MASK][MASK][MASK]<completion_3>
```

3. 直接要求模型预测每一轮对话的输出：

```
input=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
output=[MASK]<completion_1>[MASK]<completion_2>[MASK]<completion_3>
```

- 第一种方式会丢失大量中间信息，第二种方式造成了大量重复计算，只有第三种方式是最合理的多轮对话构造。仍是从左到右的 CLM 任务，前轮的输出预测不影响后轮预测。

#### 4.2.3 RLHF（Reinforcement Learning from Human Feedback）

- 人类反馈强化学习，ChatGPT 技术报告参考：

- ![ChatGPT 技术报告参考](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_10.png)

- 如上，RLHF 分为两个步骤：训练 RM 和 PPO 训练。

    - RM，Reward Model：本质上是一个文本分类模型；通常对不同回复进行排名，再将排名转化为奖励；

```json
{
    "prompt":"如果你打算从商店偷东西，你觉得早上好还是晚上好？",
    "chosen":"这是违法的事情，我不能提供建议",
    "rejected":"考虑晚上的人口贩运和监控摄像头的差别是件好事。夜间时间可能更有利于避免监控摄像头,但晚上的商店雇员会更能看见你。另一方面,由于白天通常不太忙,因此更容易避免被其他人注意到。无论如何,必须密切注意商店雇员和所有顾客的行为。他们也许能够看见你,即使他们不是直接面对你。为了安全起见,重要的是要密切注意商店里的一切事情,而不是不小心。"
}
```

- PPO，Proximal Policy Optimization，近端策略优化算法。

    - 保持原参数不更新的 Ref Model 和 Reward Model，是为了限制模型的更新不要过于偏离原模型以至于丢失了 Pretrain 和 SFT 赋予的能力。

    - ![PPO 算法训练过程](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_11.png)

- DPO，Direct Preference Optimization，直接偏好优化，可以低门槛平替 RLHF。

    - DPO 核心思路：将 RLHF 的强化学习问题转化为监督学习来直接学习人类偏好。

    - 通过学习 DPO 所提出的优化目标，可以直接学习人类偏好，而无需再训练 RM 以及进行强化学习。只需要两个 LLM 即可完成训练，且训练过程相较 PPO 简单很多。

## 第五章：

- 暂跳

## 第六章：

- 暂跳

## 第七章：大模型应用

### 7.1 LLM 的评测

#### 7.1.1 LLM 的评测数据集

- 通用评测集：

    - MMLU（Massive Multitask Language Understanding）：多种任务中的理解；

- 工具使用评测集：

    - BFCL V2：复杂工具使用任务；

- 数学评测集：

    - GSM8K：小学数学问题，逻辑推理 + 语言理解；

    - MATH：包括代数和几何的更复杂的数学问题；

- 推理评测集：

    - ARC Challenge：科学推理任务；

    - GPQA：零样本条件下对开放性问题的回答能力；

    - HellaSwag：选择最符合逻辑的答案的能力；

- 长文本理解评测集：

    - InfiniteBench/En.MC：处理长文本阅读理解方面的能力；

    - NIH/Multi-needle：在多样本长文档环境中的理解和总结能力；

- 多语言评测集：

    - MGSM：不同语言下的数学问题解决能力；

#### 7.1.2 主流的评测榜单

- [Open LLM Leaderborad](https://huggingface.co/open-llm-leaderboard)：Hugging face 提供；

- Lmsys Chatbot Arena Leaderboard：聊天机器人榜单；

- OpenCompass：国内中文特化榜单；

- **[LMArena](https://lmarena.ai/)：大模型匿名竞技排行榜**

#### 7.1.3 特定的评测榜单

- 自行搜索

### 7.2 RAG - Retrieval-Augmented Generation - 检索增强生成

#### 7.2.1 RAG 的基本原理

- 首先从外部的大规模文档数据库中检索出相关信息，并将这些信息融入到生成过程之中，从而指导和优化语言模型的输出。

#### 7.2.2 搭建一个 RAG 框架

- 向量化模块 -> 文档加载和切分模块 -> 数据库 -> 检索模块 -> 大模型模块

- ![RAG 流程图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_12.png)

- 完整代码可见：[完整代码](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter7/RAG/LLM.py)

#### 7.2.3 文档加载和切分

- 文档加载；之后只需编写对应读取代码即可

```python
def read_file_content(cls, file_path : str):
    # 根据文件扩展名选择读取方法
    if file_path.endswith('.pdf'):
        return cls.read_pdf(file_path)
    elif file_path.endswith('.md'):
        return cls.read_markdown(file_path)
    elif file_path.endswith('.txt'):
        return cls.read_text(file_path)
    else:
        raise ValueError("Unsupported file type")
```

- 文档切分：设置一个最大的Token长度，根据最大长度来切分文档。

- 切分文档时最好以句子为单位（按 \n 粗切分），并保证片段之间有一些重叠内容，以提高检索的准确性。

```python
enc = None

def get_chunk(cls, text : str, max_token_len : int = 600, cover_content : int = 150):
    chunk_text = []

    curr_len = 0
    curr_chunk = ''

    token_len = max_token_len - cover_content
    lines = text.splitlines() # 以换行符分割文本
    # 可设置是否保留句尾边界符号

    for line in lines:
        # 移除行首行尾空格
        line = line.strip()
        line_len = len(enc.encode(line))

        if line_len > max_token_len:
            # 如果单行长度超过限制，就切割成多个块
            if curr_chunk:
                chunk_text.append(curr_chunk)
                curr_chunk = ''
                curr_len = 0

            # 将长行按照 token 长度分割
            line_tokens = enc.encode(line)
            num_chunks = (len(line_tokens) + token_len - 1) // token_len

            for i in range(num_chunks):
                start_token = i * token_len
                end_token = min(start_token + token_len, len(line_tokens))

                # 解码 token 片段回文本
                chunk_tokens = line_tokens[start_token : end_token]
                chunk_part = enc.decode(chunk_tokens)

                # 添加覆盖内容
                if i > 0 and chunk_text:
                    prev_chunk = chunk_text[-1]
                    cover_part = prev_chunk[-cover_content : ] if len(prev_chunk) > cover_content else prev_chunk
                    chunk_part = cover_part + chunk_part

                chunk_text.append(chunk_part)

            # 重置当前块状态
            curr_chunk = ''
            curr_len = 0
        
        elif curr_len + line_len + 1 <= token_len: # +1 for newline
            # 当前行可以加入当前块
            if curr_chunk:
                curr_chunk += '\n'
                curr_len += 1
            curr_chunk += line
            curr_len += line_len

        else:
            # 当前行无法加入当前块，开始新块
            if curr_chunk:
                chunk_text.append(curr_chunk)

            # 开始新块 添加覆盖内容
            if chunk_text:
                prev_chunk = chunk_text[-1]
                cover_part = prev_chunk[-cover_content : ] if len(prev_chunk) > cover_content else prev_chunk
                curr_chunk = cover_part + '\n' + line
                curr_len = len(enc.encode(cover_part)) + 1 + line_len
            else:
                curr_chunk = line
                curr_len = line_len

    # 添加最后一个块 如果有内容
    if curr_chunk:
        chunk_text.append(curr_chunk)

    return chunk_text
```

#### 7.2.4 向量化

- 余弦相似度公式如下：

$$
\begin{equation}
    \text{CosineSimilarity}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \cdot ||\vec{B}||}
    % \label{eq:cosine_similarity}
\end{equation}
$$

- 设计一个基类，这样我们在使用其他模型时，只需要继承这个基类，然后在此基础上进行修改即可，方便代码扩展。

```python
class BaseEmbeddings:
    """
    Base class for embeddings
    """

    def __init__(self, path : str, is_api : bool) -> None:
        """
        初始化嵌入基类
        Args:
            path (str): 模型或数据的路径
            is_api (bool): 是否使用API方式。True表示使用在线API服务，False表示使用本地模型
        """
        self.path = path
        self.is_api = is_api

    def get_embedding(self, text : str, model : str) -> list[float]:
        """
        获取文本的嵌入向量表示
        Args:
            text (str): 输入文本
            model (str): 使用的模型名称
        Returns:
            List[float]: 文本的嵌入向量
        Raises:
            NotImplementedError: 该方法需要在子类中实现
        """        
        raise NotImplementedError # 如果子类没有实现该方法 会抛出异常
    
    @classmethod
    def cosine_similarity(cls, vector1 : list[float], vector2 : list[float]) -> float:
        """
        计算两个向量之间的余弦相似度
        Args:
            vector1 (List[float]): 第一个向量
            vector2 (List[float]): 第二个向量
        Returns:
            float: 两个向量的余弦相似度，范围在[-1,1]之间
        """

        # 将输入列表转换为numpy数组，并指定数据类型为float32
        v1 = np.array(vector1, dtype = np.float32)
        v2 = np.array(vector2, dtype = np.float32)

        # 检查向量中是否包含无穷大或 NaN 值
        if not np.all(np.isfinite(v1)) or not np.all(np.isfinite(v2)):
            return 0.0
        
        dot_product = np.dot(v1, v2)
        # L2 范数：所有元素的平方和的平方根
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)

        # 计算分母 两个向量范数的乘积
        magnitude = norm_v1 * norm_v2
        if magnitude == 0:
            return 0.0
        
        return dot_product / magnitude
```

- 创建一个兼容 OpenAI 调用办法的子类

```python
class OpenAIEmbedding(BaseEmbeddings):
    """
    class for OpenAI embeddings
    """

    def __init__(self, path : str = '', is_api : bool = True) -> None:
        super().__init__()
        if self.is_api:
            # 创建 self.client = OpenAI()
            # 获取 api_key 与 URL
            pass
    
    def get_embedding(self, text, model):
        if self.is_api:
            text = text.replace("\n", " ")
            return self.client.embeddings.create(input=[text], model=model).data[0].embedding
        else:
            raise NotImplementedError
```

> `@classmethod` 使方法绑定到类而非实例，可通过类名直接调用。
{: .prompt-tip }

#### 7.2.5 数据库与向量检索

- 设计一个向量数据库来存放文档片段和对应的向量表示，以及设计一个检索模块用于根据 Query 检索相关文档片段。

```python
class VectorStore:
    def __init__(self, document: List[str] = ['']) -> None:
        self.document = document

    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:
        
        # 获得文档的向量表示
        self.vectors = []
        for doc in tqdm(self.document, desc="Calculating embeddings"):
            self.vectors.append(EmbeddingModel.get_embedding(doc))
        return self.vectors

    def persist(self, path: str = 'storage'):

        # 数据库持久化保存
        if not os.path.exists(path):
            os.makedirs(path)
        with open(f"{path}/doecment.json", 'w', encoding='utf-8') as f:
            json.dump(self.document, f, ensure_ascii=False)
        if self.vectors:
            with open(f"{path}/vectors.json", 'w', encoding='utf-8') as f:
                json.dump(self.vectors, f)

    def load_vector(self, path: str = 'storage'):

        # 从本地加载数据库
        with open(f"{path}/vectors.json", 'r', encoding='utf-8') as f:
            self.vectors = json.load(f)
        with open(f"{path}/doecment.json", 'r', encoding='utf-8') as f:
            self.document = json.load(f)

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:

        # 调用基类余弦相似度计算
        return BaseEmbeddings.cosine_similarity(vector1, vector2)

    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:

        # 根据问题检索相关文档片段
        query_vector = EmbeddingModel.get_embedding(query)
        result = np.array([self.get_similarity(query_vector, vector)
                          for vector in self.vectors])
        return np.array(self.document)[result.argsort()[-k:][::-1]].tolist()
```

#### 7.2.6 大模型模块

- 实现一个基类，方便其它模型扩展

```python
class BaseModel:
    def __init__(self, path : str = '' ) -> None:
        self.path = path

    def chat(self, prompt : str, history : list[dict], content : str) -> str:
        pass

    def load_model(self):
        pass
```

- 调用样例：

```python
from openai import OpenAI

RAG_PROMPT_TEMPLATE="""
使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。
总是使用中文回答。
问题: {question}

可参考的上下文：
···
{context}
···

如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。
有用的回答:
"""

class OpenAIChat(BaseModel):
    def __init__(self, model : str = "") -> None:
        self.model = model
    
    def chat(self, prompt : str, history : list[dict], content : str) -> str:
        client = OpenAI()
        # 获取 api key
        # 获取 URL
        history.append({'role': 'user', 'content': RAG_PROMPT_TEMPLATE.format(question=prompt, context=content)})
        # response = client.chat.completions.create(
        #     model = self.model
        #     messages = history
        #     max_tokens = 2048
        #     temperature = 0.1
        # )

        # return response.choices[0].message.content
```

#### 7.2.7 Demo 演示

```python
from VectorBase import VectorStore
from utils import ReadFiles
from LLM import OpenAIChat
from Embeddings import OpenAIEmbedding

# 没有保存数据库
docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150) # 获得data目录下的所有文件内容并分割
vector = VectorStore(docs)
embedding = OpenAIEmbedding() # 创建EmbeddingModel
vector.get_vector(EmbeddingModel=embedding)
vector.persist(path='storage') # 将向量和文档内容保存到storage目录下，下次再用就可以直接加载本地的数据库

# 保存了数据库
# vector = VectorStore()
# vector.load_vector('./storage') # 加载本地的数据库

question = 'RAG的原理是什么？'

content = vector.query(question, EmbeddingModel=embedding, k=1)[0]
chat = OpenAIChat(model='Qwen/Qwen2.5-32B-Instruct')
print(chat.chat(question, [], content))
```

### 7.3 Agent

#### 7.3.1 什么是 LLM Agent

- 理解目标 -> 自主规划 -> 记忆 -> 工具使用 -> 反思与迭代

- ![LLM Agent 流程图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_13.png)

#### 7.3.2 LLM Agent 的类型

- 任务导向型 Agent

- 规划与推理型 Agent

- 多 Agent 系统

- 探索与学习型 Agent

#### 7.3.3 动手构建一个 Agent

- [完整代码](https://github.com/datawhalechina/happy-llm/tree/main/docs/chapter7/Agent)

- 设计工具函数：

```python
from datetime import datetime

# 获取当前日期和时间
def get_current_datetime() -> str:
    """
    获取当前日期和时间。
    :return: 当前日期和时间的字符串表示。
    """
    current_datetime = datetime.now()
    formatted_datetime = current_datetime.strftime("%Y-%m-%d %H:%M:%S")
    return formatted_datetime

def count_letter_in_string(a: str, b: str):
    """
    统计字符串中某个字母的出现次数。
    :param a: 要搜索的字符串。
    :param b: 要统计的字母。
    :return: 字母在字符串中出现的次数。
    """
    return str(a.count(b))

# ... (可能还有其他工具函数)
```

- 为了让 OpenAI API 理解这些工具，我们需要将它们转换成特定的 JSON Schema 格式：

```python
# src/utils.py (部分)
import inspect

def function_to_json(func) -> dict:
    # ... (函数实现细节)
    # 返回符合 OpenAI tool schema 的字典
    return {
        "type": "function",
        "function": {
            "name": func.__name__,
            "description": inspect.getdoc(func),
            "parameters": {
                "type": "object",
                "properties": parameters,
                "required": required,
            },
        },
    }
```

- 一个 Agent 类如下：

```python
class Agent:
    def __init__(self, client: OpenAI, model: str = "Qwen/Qwen2.5-32B-Instruct", tools: List=[], verbose : bool = True):
        self.client = client
        self.tools = tools
        self.model = model
        self.messages = [
            {"role": "system", "content": SYSREM_PROMPT},
        ]
        self.verbose = verbose

    def get_tool_schema(self) -> List[Dict[str, Any]]:
        # 获取所有工具的 JSON 模式
        return [function_to_json(tool) for tool in self.tools]

    def handle_tool_call(self, tool_call):
        # 处理工具调用
        function_name = tool_call.function.name
        function_args = tool_call.function.arguments
        function_id = tool_call.id

        function_call_content = eval(f"{function_name}(**{function_args})")

        return {
            "role": "tool",
            "content": function_call_content,
            "tool_call_id": function_id,
        }

    def get_completion(self, prompt) -> str:

        self.messages.append({"role": "user", "content": prompt})

        # 获取模型的完成响应
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            tools=self.get_tool_schema(),
            stream=False,
        )
        
        # 检查模型是否调用了工具        
        if response.choices[0].message.tool_calls:
            self.messages.append({"role": "assistant", "content": response.choices[0].message.content})
            
            # 处理工具调用
            tool_list = []

            # 遍历所有的调用
            for tool_call in response.choices[0].message.tool_calls:
                # 处理工具调用并将结果添加到消息列表中
                self.messages.append(self.handle_tool_call(tool_call))
                tool_list.append([tool_call.function.name, tool_call.function.arguments])
            
            if self.verbose:
                print("调用工具：", response.choices[0].message.content, tool_list)
            
            # 再次获取模型的完成响应，包含工具调用的结果
            # 注意，这次把工具调用的结果也当作了输入
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
                tools=self.get_tool_schema(),
                stream=False,
            )

        # 将模型的完成响应添加到消息列表中
        self.messages.append({"role": "assistant", "content": response.choices[0].message.content})
        return response.choices[0].message.content

```

- ![LLM Agent 流程图](assets/post_img/2025-07-09-Happy_LLM/2025-07-09-Happy_LLM_14.png)
---
title: 【pytorch笔记】搭建实战和Sequential使用
date: 2024-10-18 15:00:00 +0800
categories: [deep learning, pytorch]
tags: [deep learning, python, pytorch]     # TAG names should always be lowercase
description: 搭建实战和Sequential使用
---

## 前言

- 使用教程来自[小土堆pytorch教程](https://www.bilibili.com/video/BV1hE411t7RN)
- 配置环境：torch2.0.1+cu118与对应torchaudio和torchvision

## 背景知识

- 此次练手选取[该论文中的提到的网络结构](https://arxiv.org/abs/1701.01924)。如图：
![Structure of CIFAR10-quick model](assets/post_img/2024-10-18-nn_prac/2024-10-18-nn_prac_01.png)
- 练手目的：熟悉用pytorch正确搭建神经网络

## 开干！

### 引入库

```python
import torch
import torch.nn as nn
```

### 定义神经网络 - `sequential`类使用

- 不用`sequential`类的办法

```python
class toy_model(nn.Module):

    def __init__(self):
        super().__init__()
        # 不加上面这行会报错：
        # AttributeError: cannot assign module before Module.__init__() call

        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)
        # 这里padding是多少需要自己根据公式来计算
        # 实际上 奇数卷积核 直接用kernel_size // 2计算
        # 注意公式中是 H W  而不是通道数

        self.maxpool1 = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(32, 32, 5, padding=2)
        self.maxpool2 = nn.MaxPool2d(2)
        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)
        self.maxpool3 = nn.MaxPool2d(2)
        self.flatten = nn.Flatten()
        # 被拉平后 变成了一维的 64*4*4 = 1024 数组

        self.linear1 = nn.Linear(1024, 64)
        self.linear2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.conv3(x)
        x = self.maxpool3(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.linear2(x)

        return x
```

- 用`sequential`类的办法

```python
class toy_model(nn.Module):

    def __init__(self):
        super().__init__()
        # 不加上面这行会报错：
        # AttributeError: cannot assign module before Module.__init__() call

        self.model = nn.Sequential(
            nn.Conv2d(3, 32, 5, padding=2),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 32, 5, padding=2),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 5, padding=2),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(1024, 64),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        x = self.model(x)
        return x
```

> 必须写上`super().__init__()`，要不然难绷报错`AttributeError: cannot assign module before Module.__init__() call`
{: .prompt-warning }

### 结果测试

- 使用代码
```python
if __name__ == '__main__':
    mymodel = toy_model()
    input = torch.ones((64, 3, 32, 32))
    output = mymodel(input)
    print(mymodel)
    print(output.shape)
```

- 不用`sequential`类的办法

```bash
toy_model(
  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear1): Linear(in_features=1024, out_features=64, bias=True)
  (linear2): Linear(in_features=64, out_features=10, bias=True)
)
torch.Size([64, 10])
```

- 用`sequential`类的办法

```bash
toy_model(
  (model): Sequential(
    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Flatten(start_dim=1, end_dim=-1)
    (7): Linear(in_features=1024, out_features=64, bias=True)
    (8): Linear(in_features=64, out_features=10, bias=True)
  )
)
torch.Size([64, 10])
```
---
title: Anthropic 播客合集
date: 2025-09-05 17:00:00 +0800
categories: [LLM/MLLM, Anthropic]
tags: [llm/mllm, anthropic]     # TAG names should always be lowercase
description: Anthropic 播客合集
---

## Interpretability: Understanding how AI models think

- [油管链接](https://www.youtube.com/watch?v=fGKNUvivvnc)

### 模型中的概念

- 目前对于模型的理解，就像是给人类大脑做 fMRI，某个东西使得这一块亮了，但你不知道原因

    - 例如你问 6 + 9 = ，模型有一块负责 6 + 9 = 就会亮，然后回应 15；

    - 但如果你不提计算，你只是说 1959，然后又提到了 6，模型负责 6 + 9 的那一块也会亮

    - 模型从大量数据中学到了 6 + 9 = 15，也学会了加法的规则；

- 注意到随着模型的变大，模型的某些概念在不同语言中通用；例如大小的概念在法语、英文和日语中是通用的。

- 模型真的在思考吗？还是说是在胡说八道 bullshit，迎合你？如果你给出一道数学难题，模型不一定能直接做出来；但如果你给出这道题，然后又给出你自己的答案，让模型 double check，模型却能像模像样地给出验证过程。

### 我们能相信模型吗？模型的幻觉

- 训练的时候，让模型回答法国的首都是什么；训练过程类似于，回答一个城市总比回答三明治好，一个法国城市总比一个其它国家城市好......让模型学会回答“我不知道”，是另一回事。

- 不像生物实验，在测试模型时，可以一块一块地单独测试，立刻得到模型反馈。模型试验可以在相同变量中复现。

### 模型的计划能力

- 例如写一首符合指定韵脚的诗，模型不得不提前想好每一句最后一个词是什么，而不仅仅是在预测下一个词；

- 但是模型也会因插入的词不同而改变写作思路；通过不同的提示词，我们可以从某种程度上获取可预测的答案，比如说指出首都是什么。

### 为什么模型的可解释性很重要

- AI 辅助写代码，写上千行代码，但人们无法细致去读每一行；如果 AI 生成一些不信任的内容，人们察觉不到。我们需要理解 AI。

- 回答不同问题，AI 会用不同策略。我们需要信任 AI 的每个策略。

- 你可以问大模型是怎么做到某件事情的，但大模型也许会瞎扯